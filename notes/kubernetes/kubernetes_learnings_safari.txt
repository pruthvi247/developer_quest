[source : https://learning.oreilly.com/scenarios/kubernetes-fundamentals-first/9781492078821/]

kubectl version --short && \
kubectl get componentstatus && \
kubectl get nodes && \
kubectl cluster-info

helm version --short

Deployment:
----------
Deploy a simple application called echoserver:
You can administer your cluster with the kubectl CLI tool or use the visual Kubernetes Dashboard. Use this script to access the protected Dashboard. 
>>token.sh

>> kubectl create -f echoserver.yaml

A simpler way to do this is with the run command, but this only creates a Pod. We actually want a Deployment with a ReplicaSet.

kubectl run hello --image=k8s.gcr.io/echoserver:1.9 --port=8080

Notice this not only defines a kind: Deployment, but inside the deployment is a ReplicaSet of a Pod and that Pod consists of one Container. The Deployment is a preferred way of deploying applications instead of simply standing up just a Pod. You may see the advantage in the later step that scales the application.

 Ensure the Available status changes from 0 to 1.

 > kubectl get deployments,pods

 Service
The echoserver container is running in a Pod. Each Pod in Kubernetes is assigned an internal and virtual IP address at 10.xx.xx.xx. However, from outside of the cluster these IPs are not addressable, and never should be. Even within the cluster other applications normally should not attempt to address these Pods IPs. Instead each replicated Pod is fronted by a single service.

This service can be referenced by its label, and therefore access with the help of an internal Domain Name System (DNS) that will resolve the URL to the service based on the label. The Service will add a layer of indirection where it will know how to connect to the Pod. All the other applications in the cluster will connect to the service through DNS lookups and the services will connect to the specific Pods.

Expose the Pod by fronting it with a Service labeled hello.

> kubectl expose deployment hello --type=NodePort

> kubectl get service hello

The NodePort is assign a port value at some free port above 30000. For this Katacoda example we need it to be at a definitive value, here we choose 31001. Use the patch command to change the hello service NodePort from its random value to the chosen, fixed value

> kubectl patch service hello --type='json' --patch='[{"op": "replace", "path": "/spec/ports/0/nodePort", "value":31001}]'


The service NodePort is now adjusted.

>> kubectl get service hello

>> kubectl describe service hello

Because of Katacoda's virtualization you cannot address this service from your browser, but you can use Katacoda's domain as the URL to the same service. Notice the same port number placed in the subdomain of the URL.

>> curl https://2886795272-31001-kpdwevrya3.environments.katacoda.com/

Scaling:
-------

Pods are often replicated for handling parallel requests. The Service will take care of round robin load balancing across the available Pods.

We will scale the hello Pod up and down. First, in another terminal start a continuous loop that puts some load the service.
>> while true; do curl -s https://2886795272-31001-kpdwevrya3.environments.katacoda.com/ -w 'Time: %{time_total}' | grep -E 'Hostname|Time' | xargs; done

With all the curl requests in the loop, the single Pod instance is producing all the responses. However with distributed systems with a deep pool of resources it's very common to add more processes that can service multiple requests. Ask Kubernetes to scaling up the echoservice across more Pods.

>> kubectl scale deployment hello --replicas=3

Kubernetes spins up new and duplicated Pods and the same service begins to balance the requests across the pods.
>> kubectl get pods -l run=hello

The single service for these 3 pods now has the IPs of the three pods and load balances between them

>> kubectl describe service hello | grep "Endpoints"
Look at the other terminal and in a few moments the output will indicate the load balancer is rotating the requests across the three nodes.

Scale the Pods to zero and see what happens with the same top command and the requests in the other terminal.

>> kubectl scale deployment hello --replicas=0

The list will show the pods Terminating, then in a moment the list will be blank.

>> kubectl get pods

Notice while the pod count is at zero the service running in Terminal 2 is reporting no responses. Soon the above command will report No resources found.

Scale the Pods back to 1 and see how the requests are restored.

>> kubectl scale deployment hello --replicas=1

A new pod should show in a moment.
>> kubectl get deployments,pods

A few moments later the metrics will be available for the new pod.

>> kubectl get pods -l run=hello


Resilience:
----------

Applications designed to embrace failure are the most healthy and resistant to interrupting users' experiences. Kubernetes embraces failure and so should your applications. One failure to expect is your containers will fail. Kubernetes expects containers to crash and will restart Pods when that happens. When your application is load balancing and spreading fault tolerance across multiple Pods, whole groups of users should rarely be effected by container failures.

You can witness Kubernetes resilience feature by purposefully killing your Pods. Scale the Pods back up.

>> kubectl scale deployment hello --replicas=3

Get the list of running pods.

>> kubectl get pods --selector=run=hello

Get the list of running pods.

>> kubectl get pods --selector=run=hello

Delete one of the Pods.
>> kubectl delete --now pod $(kubectl get pods --selector=run=hello | sed '2!d' | cut -d' ' -f1) > /dev/null &

When a Pod is no longer running, the Kubernetes controller recognizes the different between the declared state and the reality of the cluster state. The controller will instruct the Scheduler on how to resolve the problem and the Schedular will search out the most healthy and optimum Kubelet among of the worker nodes. The Kubelet will start the new Pod. Shortly thereafter the Controller will recognize the state of the cluster now matches the declared state and peace is restored.


Rollout:
--------
If you inspect the Pod you will see the running container is version 1.9.


>> kubectl describe pod hello | grep "Image:"

A newer version of the container is 1.10 as listed here (image registry).

An important aspect of Kubernetes is your users may benefit from the ideas of continuous deployment. A fundamental way to approach this is with Kubernetes rollouts.


Here are two approaches.

1. A precise surgical way is with the set image command. This will modify the image version for the container in each Pod.

>> kubectl set image deployment/hello hello=k8s.gcr.io/echoserver:1.10 --all

Now, the Pod inspection will report the updated container.

>> kubectl describe pod hello | grep "Image:"

2. Another way is to modify the YAML then apply the change with the replace command.

After trying the above set image change the image back using the replace which will move the container back to version 1.9 as specified in the YAML.

>> kubectl replace -f echoserver.yaml

Verify the version has been restored to 1.9.

>> kubectl describe pod hello | grep "Image:"


Then, look up the resource, change the image version with SED, then pipe modified stream to the replace command.

>> kubectl get deployment hello -o yaml | sed 's/\(echoserver\):.*$/\1:1.10/' | kubectl replace -f -

Verify the version has been upgraded using the replace command.

>> kubectl describe pod hello | grep "Image:"


Once an application is in a container, it's fairly simple to ask Kubernetes to serve up the container in multiple Pods fronted with a load balancing Service. Thereafter, Kubernetes dutifully respects your declared request by ensuring the application remains running on the cluster.

There are many other resources that can be added to Kubernetes besides just Pods, Services and Deployments, however, these are the most common resources. Later, explore the Helm scenarios to see how whole charts of more complex deployments can be installed.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


[source : https://learning.oreilly.com/live-training/courses/kubernetes-in-three-weeks/0636920446392/] - safari online learning





More than 20 years ago Peter Deutsch and James Gosling defined the 8 fallacies of distributed computing. These are false assumptions that many developers make about distributed systems. These are usually proven wrong in the long run, leading to hard to fix bugs.

The 8 fallacies are:

The network is reliable.
Latency is zero.
Bandwidth is infinite.
The network is secure.
Topology doesn't change.
There is one administrator.
Transport cost is zero.
The network is homogeneous.


> blogs : https://github.com/kelseyhightower/kubernetes-the-hard-way


Kubernetes 16 funtions:

> security: we can acheive thorough rbac,meshing (ssl),namespace (add boundaries between namespaces)

> volume: pod runs on node,node has volume and if we want to pesist volumes to external storage - s3,filemounts,blob

> Balancing : we can put service in front of pod so we can blance traffic between pods

> portability : 

> Resources : kube manages resources for you like compute,cpu,storage

> Sheculing : kube will check where to run the application in the cluster and it manages all the communication between nodes

> kubernetes is declerative(abstraction),distributed, extensible(we can extend same kube for custom functions-needs),(observability - monitoring-logging,tracing,metrics)

> kube has networking layer

>  self healing: when pod goes down(fail) it will bring up automatically

> name space : kafka we can put in name space, DB another name space,QA name space,DEV namespace. we can add security walls for each name space

Master node: Master node runs the process that manages the cluster, we should have two masters atleast for high availability.
worker nodes: worker nodes run the porcess
pods: pods are grouping of one or more containers. why should we run more than one container ??
Services: load balancing between replicated pods (round robin), one pod will not directly talk to another pod,it communicates thorough services

> what makes a worker as worker node is it has kubelet which is linux process written in go, main job of kubelet is to execute the instructions given by master node and also it is responsible of life cycle of pods,reports the state of the pods

> etcd: is brain of the kubernetes, it maitains the state of the cluster, all the informations shared by kubelet is stored in etcd, works on raft consensus algorithm.it should be highly available

> scheduler: will work on the health of all the kubelets and manage the running of the pods based on the health of the node, in crisp it will find the space to run the pod,scheduler will allocated the pod details but it will not start the pods, kubelet will start the pods and communicates the status of pods to master (controller manager) 

> control manager: when we say 100 pods running, contorller know how to manage them, it will works with scheduler to bring up 100 pods if its not up and running, will keep pinging pods in loop and check if all the pods are up.

>  proxy: outside customers will talk to kube cluster through ingress. traffic from ingress will travel through pod services, its like routing tables

> every time kubelet creates a POD, that pod will be assigned a new ip address(virtual ip)

> kubelet starts and stops the pod in cluster, kubelet doesnt have any code to start of stop the containers/pods , it talks thorough CRI-container runtime interface 

> Helm: package manager for kubenates, pack group of yaml files

> when we create a pod we can put a service in front of it

> probes: liveness probe,startup probe,readiness probe 



> kubernetes scenarios:

1> kubernetes fundamentals first kube application

>> kubectl get namespaces
>> kubectl get pods -n <namespace>



2> scenario : [source :https://learning.oreilly.com/scenarios/kubernetes-applications-nginx/9781492078890/]

ngnix:

>> 
>> In the first deployment we simply pass a few parameters that declare to Kubernetes our intent to make Nginx available. Use this command.

kubectl run nginx-one --image=nginx --port=80

from outside of Kubernetes at this terminal, it cannot be easily reached. Let's front the Pod with a Service. The service type will be NodePort which will expose the service on a high, random port.

>> kubectl expose pod nginx-one --type=NodePort

check the service list and notice the nginx-one service now is listed with a high port number.

>> kubectl get services

The service is assigned a random Kubernetes NodePort (some value above 30000) and this next line will force the NodePort to 31111

>> 
>> kubectl patch service nginx-one --type='json' --patch='[{"op": "replace", "path": "/spec/ports/0/nodePort", "value":31111}]'

Deploy Nginx with Manifest Technique:
Kubernetes relies on manifests as declarations for the desired state of the cluster. When you submit a manifest, the Kubernetes components such as the controller, scheduler, and Kubelets will busily help you by accepting this new state and ensure the cluster matches the declaration you specified. Nothing you do with Kubernetes is scripted with long and imperative scripts. Instead, a series of declarative manifests simply state the truth and the Kubernetes reconciliation engine (controller) will ensure your statements of truth matches the reality of the cluster. If it does not, notifications can be queried or published.


controlplane $ cat nginx.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-two-deployment
spec:
  selector:
    matchLabels:
      app: nginx-two
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx-two
    spec:
      containers:
      - name: nginx-two
        image: nginx:1.17-alpine
        ports:
        - containerPort: 80
          name: nginx-pod-port
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-two
  labels:
    app: nginx-two
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: nginx-pod-port
    nodePort: 31112
    protocol: TCP
  selector:
    app: nginx-two

Typically manifests are sources stored in version control in YAML file format. Kubernetes also accepts manifests in JSON form but since we often edit these files, the YAML format tends to be easier to read and edit than JSON. These source files are examples of infrastructure-as-code (IaC).

Let's send this manifest declaration to our Kubernetes cluster.

>> kubectl apply -f nginx.yaml


Notice the manifest defines both a Kind: Deployment and a Kind: Service. The Deployment defines the Nginx in a container and in a Pod. The Service provides an access point and load balancer in front of the Pod. This specific deployment has three Pods specified.

As we did in the previous step, inspect the starting resources. This time they are named nginx-two.

>> kubectl get deployments,replicasets,pods,services.
>> kubectl get pods -A


>> scenario : [source: https://learning.oreilly.com/scenarios/kubernetes-applications-rabbitmq/9781492078913/]

RabbitMQ is an open-source message broker software (sometimes called message-oriented middleware) that originally implemented the Advanced Message Queuing Protocol (AMQP) and has since been extended with a plug-in architecture to support Streaming Text Oriented Messaging Protocol (STOMP), Message Queuing Telemetry Transport (MQTT), and other protocols.

The RabbitMQ server program is written in the Erlang programming language and is built on the Open Telecom Platform framework for clustering and failover.

Deploy RabbitMQ
Create a namespace for the installation target.

>> kubectl create namespace rabbit


Add the chart repository for the Helm chart to be installed.

>> helm search repo rabbitmq

>> helm repo add stable https://kubernetes-charts.storage.googleapis.com

Provisioning RabbitMQ on Kubernetes is easy, just install this Helm chart.

>> helm install my-rabbit stable/rabbitmq-ha \
  --version 1.46.1 \
  --namespace rabbit \
  -f rabbit-values.yaml

The RabbitMQ containers take a few moments to start. To get a complete status of provisioning this sequence, run this inspection.

>> watch kubectl get services,statefulsets,pods --namespace rabbit

In a few moments the 3 StatefulSet Pods (RabbitMQ Nodes) labeled pod/my-rabbit-rabbitmq-ha-[0|1|2] will appear and move to the Running status. Once all are running, discontinue the watch.


>> controlplane $ cat rabbit-values.yaml
# Customize of rabbitmq-ha chart found here:# https://github.com/helm/charts/tree/master/stable/rabbitmq-ha# Requested Rabbit MQ "nodes" (StatefulSet Pods), not to be confused with Kubernetes "nodes".replicaCount: 3
# Exposed dashboard as NodePort for demonstration purposes.
service:
  type: NodePort
  managerNodePort: 31000

# Admin credentials, use these for sign in to RabbitMQ dashboard
rabbitmqUsername: guest
rabbitmqPassword: guest

# No monitoring at the moment, but may add soon
prometheus:
  operator:
    enabled: false

# A blank probe disables the default probe. Keeps false failing for some reason.
readinessProbe:


> restarting the pod with same ip address is statefulset

Kubernetes fundamentals: configmaps and secrets:
-----------------------------------------------

[source : https://learning.oreilly.com/scenarios/kubernetes-fundamentals-configmaps/9781492078869/]

> Configuration that varies between deployments should be stored in the environment.Anything that is configurable, changeable or varies between contexts should be submitted separately for each deployment. 

> A core component of the Kubernetes management plane is etcd. Etcd is a high-available, distributed key/value store ideal for contextual environment settings and data. The ConfigMaps and Secrets are simply interfaces for managing this information in etcd.

Pods provide containerized applications access to ConfigMaps and Secrets with three techniques:

1)command-line arguments
2)Environment variables
3)Files in a volume

Create
Before containers can consume contextual configuration data, that data must first be created and stored in ConfigMaps. In later steps the same will be done slightly differently for Secrets.

A ConfigMap is simple data associated with a unique key.

Create ConfigMap from CLI:
Use the kubectl tool to create two manually entered data items with a key called mountains.

>> kubectl create configmap mountains --from-literal=aKey=aValue --from-literal=14ers=www.14ers.com
Check Kubernetes using get after the create and it will indicate 2 for DATA associated with mountains.

>> kubectl get configmap mountains

To see the actual data, get it in YAML form.

>> kubectl get configmap mountains -o yaml
or in description form
>> kubectl describe configmap mountains

Finally, to complete CRUD operations delete the mountains.

>> kubectl delete configmap mountains

Create from YAML
A better way to define ConfigMaps is with a resource YAML file in this form.

cat ucs-org.yaml
>>>
apiVersion: v1
kind: ConfigMap
metadata:
  name: ucs-info
  namespace: default
data:
  property.1: hello
  property.2: world
  ucs-org: |-
    description="Our scientists and engineers develop and implement innovative, practical solutions to some of our planet's most pressing problems"
    formation=1969
    headquarters="Cambridge, Massachusetts, US"
    membership="over 200,000"
    director="Kathleen Rest"
    president="Kenneth Kimmell"
    founder="Kurt Gottfried"
    concerns="Global warming and developing sustainable ways to feed, power, and transport ourselves, to fighting misinformation, advancing racial equity, and reducing the threat of nuclear war."
    website="ucsusa.org"

>> kubectl create -f ucs-org.yaml


>> kubectl describe configmap ucs-info




Three Access Techniques:
Once the configuration data is stored in ConfigMaps, the containers can access the data. Pods grant their containers access to the ConfigMaps through these three techniques:

> through the application command-line arguments,
> through the system environment variables accessible by the application,
> through a specific read-only file accessible by the application.
The next steps, explore these access techniques.

Command Line Arguments
This example shows how a Pod accesses configuration data from the ConfigMap by passing in the data through the command-line arguments when running the container. Upon startup, the application would reference these parameters from the program's command-line arguments.

View the resource definition.

cat consume-via-cli.yaml

>>>
apiVersion: v1
kind: Podmetadata:
  name: consume-via-cli
spec:
  containers:
    - name: consuming-container      image: k8s.gcr.io/busybox
      command: [ "/bin/sh", "-c", "echo $(PROPERTY_ONE_KEY); echo $(UCS_INFO); env" ]
      env:
        - name: PROPERTY_ONE_KEY
          valueFrom:
            configMapKeyRef:
              name: ucs-info
              key: property.1        - name: UCS_INFO          valueFrom:            configMapKeyRef:
              name: ucs-info
              key: ucs-org
  restartPolicy: Never

Create the Pod.

>> kubectl create -f consume-via-cli.yaml

Environment Variables:
This example shows how a Pod accesses configuration data from the ConfigMap by passing in the data as environmental parameters of the container. Upon startup, the application would reference these parameters as system environment variables.

View the resource definition.

cat consume-via-env.yaml
>>>
apiVersion: v1
kind: Podmetadata:
  name: consume-via-env
spec:
  containers:
    - name: consuming-container      image: k8s.gcr.io/busybox
      command: [ "/bin/sh", "-c", "env" ]
      envFrom:
      - configMapRef:
          name: ucs-info
  restartPolicy: Never

Create the Pod.

>> kubectl create -f consume-via-env.yaml

Once the pod starts, its log can be viewed. The container in the Pod has written its environment variables to the console, which is now visible in the Pod's log.

>> kubectl logs consume-via-cli

Volume Mounts:
This example shows how a Pod accesses configuration data from the ConfigMap by reading from a file in a directory of the container. Upon startup, the application would reference these parameters by referencing the named files in the known directory.

View the resource definition.

cat consume-via-vol.yaml
>>>
apiVersion: v1
kind: Pod
metadata:
  name: consume-via-vol
spec:  containers:
    - name: consuming-container
      image: k8s.gcr.io/busybox
      command: [ "/bin/sh","-c","cat /etc/config/keys" ]
      volumeMounts:      - name: config-volume
        mountPath: /etc/config
  volumes:
    - name: config-volume
      configMap:
        name: ucs-info
        items:
        - key: ucs-org          path: keys
  restartPolicy: Never

>> kubectl create -f consume-via-vol.yaml


Create Secret
Both ConfigMaps and Secrets are stored in etcd, but the way you submit secrets is slightly different than ConfigMaps.

Create from CLI
>> kubectl create secret generic db-password --from-literal=password=MyDbPassw0rd

>> kubectl get secret db-password

>> kubectl get secret db-password -o yaml

Notice when the secret is returned it's not the same string that was passed in, password: TXlEYlBhc3N3MHJk. It is the same data, just encoded in 64bit form. Kubernetes stores the data assigned to a secret in 64-bit form. Never confuse encoding with encryption, as they are two very different concepts. Values encoded are not encrypted. The encoding is to allow a wider variety of values for secrets. You can easily decode the text with a simple base64 command to reveal the original password text.

>> echo "TXlEYlBhc3N3MHJkCg==" | base64 --decode or kubectl get secrets db-password -o 'go-template={{index .data "password"}}' | base64 --decode


Create from YAML
A better way to define Secrets is with a resource YAML file in this form.

kubectl create -f secret.yaml

Look inside the YAML.

cat secret.yaml
>>>
apiVersion: v1
kind: Secret
metadata:
  name: db-creds
type: Opaque
data:
  username: dXNlcgo=
  password: TXlEYlBhc3N3MHJkCg==


When first creating the YAML file you can skip using the base64 command and instead use the kubectl --dry-run feature which will generate the YAML file for you with the encoding.

>> kubectl create secret generic db-password --from-literal=password=MyDbPassw0rd --dry-run -o yaml > my-secret.yaml

cat my-secret.yaml
>>>
apiVersion: v1
data:
  password: TXlEYlBhc3N3MHJk
kind: Secret
metadata:
  creationTimestamp: null
  name: db-password


Read Secret:
Just as there are three ways to get ConfigMap data into a container, the same three techniques are available for Secrets. This step covers secrets supplied to containers as environment properties.
cat kuard.yaml
>>>
apiVersion: v1
kind: Pod
metadata:
  name: kuard
  labels:
    app: kuard
spec:
  containers:
  - image: gcr.io/kuar-demo/kuard-amd64:1
    name: kuard
    ports:
    - containerPort: 8080
      name: http
      protocol: TCP
    env:
    - name: SECRET_USERNAME
      valueFrom:
        secretKeyRef:
          name: db-creds
          key: username
    - name: SECRET_PASSWORD
      valueFrom:
        secretKeyRef:
          name: db-creds
          key: password
Launch the Kuard application and associate a service with its Pod.

>> kubectl create -f kuard.yaml
>> kubectl create -f kuard-service.yaml

[useful source : https://github.com/bitnami-labs/sealed-secrets, https://kubernetes.io/blog/2018/07/18/11-ways-not-to-get-hacked/#4-separate-and-firewall-your-etcd-cluster, https://redlock.io/blog/cryptojacking-tesla]

> enable RBAC and protect your Kubernetes API. Unprotected access to the cluster, such as through the dashboard, can unveil secrets. Invest in protecting your Kubernetes cluster and avoid what others have done in the past

[source : https://learning.oreilly.com/scenarios/kubernetes-fundamentals-jobs/9781492078852/]

Kubernetes: job resources:
-------------------------

> A Job creates Pods that run until successful termination (i.e., exit with 0). In contrast, a regular Pod will continually restart regardless of its exit code. Jobs are useful for things you only want to do once, such as database migrations or batch jobs. If run as a regular Pod, your database migration task would run in a loop, continually repopulating the database after every exit.

The Job object is responsible for creating and managing pods defined in a template in the Job specification. These pods generally run until successful completion. The Job object coordinates running multiple pods in parallel.


> Run Single Job
This scenario is based on the Chapter 12, Jobs, Kubernetes Up & Running, 2nd Edition. The chapter starts with a simple kubectl run command to start a Job. However, this Katacoda scenario uses Kubernetes 1.18. Active software will naturally evolve. Kubernetes is evolving and there was a decision in 1.18 to:

Remove all the generators from kubectl run. It will now only create pods. Additionally, deprecates all the flags that are not relevant anymore. -- 1.18 Release Notes, Deprecation

So this will no longer start a Job, it starts a Pod. Let's do it anyways see what happens.

>> kubectl run -i oneshot \
  --image=gcr.io/kuar-demo/kuard-amd64:blue \
  --restart=OnFailure \
  -- \
     /kuard \
     --keygen-enable \
     --keygen-exit-on-complete \
     --keygen-num-to-gen 5
Once complete you will see no Jobs where started.

>> kubectl get jobs
Instead, a single Pod was started and the application ran to completion performing its generation of ten keys.
If this was a Job it would have completed and cleaned up after itself. Since it's just a Pod, you now have garbage to clean up.

>> kubectl delete pod oneshot
[source useful: https://github.com/kubernetes-up-and-running/kuard/issues/33]

The above kubectl run command also differs from the book as an extra parameter needs to be passed to the container. The extra parameter on the 5th line, /kuard was added.

Job from Resource
It's generally preferable to define resources such as Jobs in resource manifests such as this YAML file.

>> curl https://raw.githubusercontent.com/kubernetes-up-and-running/examples/master/10-1-job-oneshot.yaml

>>>
apiVersion: batch/v1
kind: Jobmetadata:
  name: oneshot
  labels:
    chapter: jobs
spec:
  template:
    metadata:
      labels:
        chapter: jobs
    spec:
      containers:
      - name: kuard
        image: gcr.io/kuar-demo/kuard-amd64:1
        imagePullPolicy: Always
        args:
        - "--keygen-enable"        - "--keygen-exit-on-complete"        - "--keygen-num-to-gen=10"      restartPolicy: OnFailure

To invoke this Job declaration use the apply command.

>> kubectl apply -f https://raw.githubusercontent.com/kubernetes-up-and-running/examples/master/10-1-job-oneshot.yaml
>> kubectl describe jobs oneshot

> When Pods run the output from the containers is captured in the Pod's /var/log directory. If you know the name of the specific job instance, the logs command can extract that log. With a little help from grep and cut the name of the job is extracted and passed to the logs command

>> export JOB_ID=$(kubectl describe jobs oneshot | grep -o 'Created pod: .*' | cut -f3- -d' ')
>> echo $JOB_ID
>> kubectl logs $JOB_ID

Once you are done inspecting the job, use the delete command to remove it.
>> kubectl delete jobs oneshot

Job Failure
Failure is embraced as a first-class visitor in Kubernetes. All things are expected to fail at some point. Let's see the behavior when a job fails.
By default, the kaurd container completes with an success exit code of 0. A command can be passed to force a different exit code. Inspect this job definition that forces the container to end with an exit code of 1 after generating 3 keys.

>> curl https://raw.githubusercontent.com/kubernetes-up-and-running/examples/master/10-2-job-oneshot-failure1.yaml
>>>
apiVersion: batch/v1
kind: Job
metadata:
  name: oneshot
  labels:
    chapter: jobs
spec:
  template:
    metadata:
      labels:
        chapter: jobs
    spec:
      containers:
      - name: kuard
        image: gcr.io/kuar-demo/kuard-amd64:1
        imagePullPolicy: Always
        args:
        - "--keygen-enable"
        - "--keygen-exit-on-complete"
        - "--keygen-exit-code=1"
        - "--keygen-num-to-gen=3"
      restartPolicy: OnFailure

Run the job and see it fail.

>> kubectl apply -f https://raw.githubusercontent.com/kubernetes-up-and-running/examples/master/10-2-job-oneshot-failure1.yaml


After it runs for a moment, inspect the Pod status.

>> kubectl get pod --selector job-name=oneshot

In a moment, notice the column RESTARTS reports Kubernetes is attempting to re-run the job in hopes the subsequent executions will pass.

This is because the job was submitted with the restartPolicy: OnFailure setting. Kubernetes will continue to repeat the lifecycle of the Job. Alternatively, the policy could have been set to Never to prevent the restarts.

Cleanup this job with the delete command.

>> kubectl delete jobs oneshot

Parallelism:
A huge advantage of running applications on Kubernetes is there are typically large amounts of resources to utilize. So far these jobs have been working serially and underutilizing the resources. Often real jobs may be long-running and incur notable CPU and memory consumption. A helpful way to solve performance problems is by running the jobs in parallel. Instead of each job creating a series of keys serially,3 have multiple jobs work on smaller units of work.

A Race
Let's have a race to validate this claim, even on this limited Katacoda Kubernetes instance.

Serially
First, run the job we ran in the previous step 2 and have it generate 10 keys.

kubectl apply -f https://raw.githubusercontent.com/kubernetes-up-and-running/examples/master/10-1-job-oneshot.yaml

This time we will watch for is the duration result. By inspecting the status times in the job's YAML the duration time can be extracted. It will be about 30 seconds before the end time is recorded. If a message:

date: invalid date ‘null’
expr: syntax error
appears, it just means the job is not done and the completionTime has not been recorded yet.

>> echo "Duration: $(expr $(date +%s -d $(kubectl get job oneshot -o json | jq -r .status.completionTime)) - $(date +%s -d $(kubectl get job oneshot -o json | jq -r .status.startTime))) seconds"

Once the seconds value appears, take note of it.

>> export SERIAL_DURATION=$(expr $(date +%s -d $(kubectl get job oneshot -o json | jq -r .status.completionTime)) - $(date +%s -d $(kubectl get job oneshot -o json | jq -r .status.startTime)))
>> echo $SERIAL_DURATION


Parallel
Here is a descriptor that will run the job in parallel. Inspect the definition

>> curl https://raw.githubusercontent.com/kubernetes-up-and-running/examples/master/10-3-job-parallel.yaml
>>>
apiVersion: batch/v1
kind: Job
metadata:
  name: parallel
  labels:
    chapter: jobs
spec:
  parallelism: 5  completions: 10
  template:
    metadata:
      labels:
        chapter: jobs
    spec:
      containers:
      - name: kuard
        image: gcr.io/kuar-demo/kuard-amd64:1
        imagePullPolicy: Always
        args:
        - "--keygen-enable"        - "--keygen-exit-on-complete"
        - "--keygen-num-to-gen=10"
      restartPolicy: OnFailure


Notice in the spec: the two settings have been applied to ensure the parallelism features is enabled parallelism: 5 and completions: 10.

The template runs 10 jobs where each job generates 10 keys. For a better race comparison, we want 10 jobs that each generate a single key, so the template will be slightly modified using sed to change the keygen-num-to-gen value from 10 to 1.

>> curl https://raw.githubusercontent.com/kubernetes-up-and-running/examples/master/10-3-job-parallel.yaml | sed '/num-to-gen=/s/=.*/=1"/' > job-parallel.yaml


Notice the keygen-num-to-gen setting is set to =1.

>> ccat job-parallel.yaml
>>>
apiVersion: batch/v1
kind: Job
metadata:
  name: parallel
  labels:
    chapter: jobs
spec:
  parallelism: 5
  completions: 10
  template:
    metadata:
      labels:
        chapter: jobs
    spec:
      containers:
      - name: kuard
        image: gcr.io/kuar-demo/kuard-amd64:1
        imagePullPolicy: Always
        args:
        - "--keygen-enable"
        - "--keygen-exit-on-complete"
        - "--keygen-num-to-gen=1"
      restartPolicy: OnFailure

>> kubectl apply -f job-parallel.yaml
Again, by inspecting the status times in the job's YAML the duration time can be extracted. It will be less than 30 seconds before the end time is recorded. If a message:

date: invalid date ‘null’
expr: syntax error
appears, it just means the job is not done and the completionTime has not been recorded yet.

>> echo "Duration: $(expr $(date +%s -d $(kubectl get job parallel -o json | jq -r .status.completionTime)) - $(date +%s -d $(kubectl get job parallel -o json | jq -r .status.startTime))) seconds"

Once the seconds value appears, take note of it.

>> export PARALLEL_DURATION=$(expr $(date +%s -d $(kubectl get job parallel -o json | jq -r .status.completionTime)) - $(date +%s -d $(kubectl get job parallel -o json | jq -r .status.startTime)))

>> echo $PARALLEL_DURATION

Finally, delete the 2 jobs by filtering on a label.

>> kubectl delete jobs --selector chapter=jobs

However, to fully take advantage of parallelism some asynchronous message queuing should also be woven into the mix. In the next step, you will leverage a queue.

Work Queue:
Jobs are an effective mechanism to accept work from a queue and publish the results downstream when completed. Multiple Jobs can run asynchronously and in parallel to accept enqueued items and deque the items when completed.

This example starts up a simple queuing service, enqueues keygen request work items, then parallel jobs process the work items by submitting they keygen results back to the queue.

Start Queuing Service
Start the work queue.

cat queue.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  labels:
    app: work-queue
    component: queue
    chapter: jobs
  name: queue
spec:
  replicas: 1
  selector:
    matchLabels:
      app: work-queue
  template:
    metadata:
      labels:
        app: work-queue
        component: queue
        chapter: jobs
    spec:
      containers:
      - name: queue
        image: "gcr.io/kuar-demo/kuard-amd64:1"
        imagePullPolicy: Alwayscontrolplane

Start Queuing Service
Start the work queue.

>> kubectl apply -f queue.yaml

Submit Items to Queue
Create a collection of work items and place onto queue. First, use port forwarding locally as the shell script expects the queue service to be available on port 8080.

>> QUEUE_POD=$(kubectl get pods -l app=work-queue,component=queue -o jsonpath='{.items[0].metadata.name}')
>> kubectl port-forward $QUEUE_POD 8080:8080 > /dev/null &

After the port-forward command press enter to restore the command prompt.

>> curl https://raw.githubusercontent.com/kubernetes-up-and-running/examples/master/10-6-load-queue.sh | bash
>>>
# Create a work queue called 'keygen'
curl -X PUT localhost:8080/memq/server/queues/keygen

# Create 100 work items and load up the queue.
for i in work-item-{0..99}; do
  curl -X POST localhost:8080/memq/server/queues/keygen/enqueue \
    -d "$i"
done

View the enqueued items count, it will be 100.

>> curl localhost:8080/memq/server/stats | jq


You can also see the queue in the portal. Because of Katacoda's virtualization, you cannot address this URL from your browser, but you can use Katacoda's domain as the URL to the same service.

These commands will expose the service externally to Katacoda's subdomain scheme.

Expose the queue with a Kubernetes service to allow access to the Queue Portal.

>> kubectl apply -f https://raw.githubusercontent.com/kubernetes-up-and-running/examples/master/10-5-service-queue.yaml
>>>
apiVersion: v1
kind: Service
metadata:
  labels:
    app: work-queue
    component: queue
    chapter: jobs
  name: queue
spec:
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    app: work-queue
    component: queue
>> kubectl patch service queue --type='json' --patch='[{"op": "replace", "path": "/spec/type","value":"NodePort"}]'

>> kubectl patch service queue --type='json' --patch='[{"op": "replace", "path":"/spec/ports/0/nodePort", "value":31001}]'

Notice the same port number is placed in the subdomain of the URL.

>> https://2886795298-31001-host11nc.environments.katacoda.com/

Process Work Items
Inspect the new Jobs that will consume and work on these enqueued items. Notice the resource is defined with 5 jobs to run in parallel. Once all the queued items have been processed the jobs will complete.

>> curl https://raw.githubusercontent.com/kubernetes-up-and-running/examples/master/10-7-job-consumers.yaml
>>>
apiVersion: batch/v1
kind: Job
metadata:
  labels:
    app: message-queue
    component: consumer
    chapter: jobs
  name: consumers
spec:
  parallelism: 5
  template:
    metadata:
      labels:
        app: message-queue
        component: consumer
        chapter: jobs
    spec:
      containers:
      - name: worker
        image: "gcr.io/kuar-demo/kuard-amd64:1"
        imagePullPolicy: Always
        args:
        - "--keygen-enable"
        - "--keygen-exit-on-complete"
        - "--keygen-memq-server=http://queue:8080/memq/server"
        - "--keygen-memq-queue=keygen"
      restartPolicy: OnFailure


Create a parallel consumer Job.
>> kubectl apply -f https://raw.githubusercontent.com/kubernetes-up-and-running/examples/master/10-7-job-consumers.yaml

Watch the activity of pods, queue, and Kubernetes dashboard.

>> kubectl get pods

Go back to the Queue portal watch the items get processed until all 100 are complete. You can also watch the progress with this curl.

>> watch -n 3 curl -s https://2886795298-31001-host11nc.environments.katacoda.com/memq/server/stats
Once you are done watching the progress, use this clear to break out of the watch or press ctrl+c

Clean Up
Remove the queue (ReplicaSet and Service) and the Jobs.

>> kubectl delete rs,svc,job --selector chapter=jobs


Jobs are much like Pods, but differ since they terminate once the task is completed. The Job feature ensures the job complete successfully and can optionally rerun the tasks until success is reported.

Job efficiently run in parallel. Where once you may have been inclined to multi-thread the application in the container, you can instead keep the container a simple task runner and rely on Kubernetes resource management and Job parallelism to achieve threading with a more effective pattern.

When combined with a queuing or messaging mechanism, Jobs can asynchronously process any tasks you decide to design.

[source : https://learning.oreilly.com/live-training/courses/kubernetes-in-three-weeks/0636920446392/]

Session 2 of kubernetes in 3weeks:
---------------------------------
Terminology:

Container image: A file of bytes following the container image format that defines the container.Stored in a repository often composed of multiple layers.

Image layer: container images are typically many layers of images.Each layer defines distinct and reusable container definitions.FROM,COPY,ADD AND RUN add new layers

container engine: Process on the host OS that accepts requests to pull images,calls the container runtime and cleans up ate containers terminate.

registry : library or datastore of multiple repositories hosts and manages the repo files and manifests(add,remove,list security)

Repository: listing of one or more versions of a container along with its images layers and manifests.Follows established image manifest schema

Tag: A label that uniquely identifies multiple container images in a single repository

BaseImage: Most containers build up on base container images layers>and build more layers on top of them.Encourages reuse

> There are 17 instructions in docker:
common instructions:

FROM,RUN,COPY,CMD,ENTRYPONT

Secondary instructions:

LABEL,EXPOSE,ENV,ADD,VOLUME,USER,WORKDIR,ARG,ONBUILD,STOPSIGNAL,HEALTHCHECK,SHELL
 
>> docker pull redis
>> docker save --output redis.tar redis (this will save the docker file in .tar format in filesystem)

[source: https://learning.oreilly.com/scenarios/kubernetes-containers-decomposing/9781492090137/]

>Container Image Dissection
A container image is a TAR file containing other TAR files. Internally, each TAR file is a layer. Once all TAR files have been extracted to a local filesystem, you can explore the details of the layers.

Using the docker tool, pull the layers of a Redis container image onto this filesystem.

>> docker pull redis:6.0.4-alpine3.11

Export the image into a raw TAR format.

>> docker save redis:6.0.4-alpine3.11 > redis.tar

Create a scratch location to inspect the Redis files.

>> mkdir redis && cd redis

Extract the files from the TAR.

>> tar -xvf ../redis.tar

All of the contents, along with the layer TAR files, are now viewable.

>> tree

The image includes the manifest.json file that defines the metadata about the image, such as version information and tag names. The schema for the manifest.json file follows the OCI specification. Inspect the manifest.

>> cat manifest.json | jq .

Extracting a layer will reveal the specific files contained for that layer.

>> mkdir last-layer && tar -xvf 014d4966196e17dec4032a93660d4be192558c0a654af6347a6e012742079d6c/layer.tar -C last-layer

Inspect the files in the last layer.

>> tree last-layer

This single file makes sense because it's the last instruction in the Redis Dockfile that would cause a layer to be created https://github.com/docker-library/redis/blob/master/6.0/Dockerfile#L101


Creating an Empty Image
A container image is a TAR of TAR files with some metadata. (Tartar sauce?) Therefore, an empty image can be created using the tar command below.

>> tar cv --files-from /dev/null | docker import - empty

When importing the TAR file, the docker tool will add the required metadata. The command creates the image and reports the image ID, and is now in the images list.

>> docker images

The container image is ready, but as the container doesn't contain anything, it can't start a process.

>> docker run empty

The container runtime appropriately reports:

No command specified.

Create Image without Dockerfile
The previous idea of importing a TAR file can be extended to create an entire image from scratch.

Next, we'll use BusyBox as the base to create a functional container by just using this tar and import technique. BusyBox combines tiny versions of many common UNIX utilities into a single small executable. Install BusyBox locally.

>> apt install busybox-static

Docker provides a script to download the BusyBox rootfs.

>> curl -LO https://raw.githubusercontent.com/moby/moby/a575b0b1384b2ba89b79cbd7e770fbeb616758b3/contrib/mkimage/busybox-static && chmod +x busybox-static

>> ./busybox-static busybox

Running the script will download the rootfs and the main binaries.

>> ls -lha busybox

The default Busybox rootfs doesn't include any version information, so let's create a file.

>> echo KatacodaPrivateBuild > busybox/release

As before, the directory can be converted into a TAR and automatically imported into Docker as an image.

>> tar -C busybox -c . | docker import - busybox

You have created a container image.

>> docker images

Finally, your new container image can be used to launch your custom BusyBox container.

>> docker run busybox cat /release

All those BusyBox commands are also available.

>> docker run busybox /bin/sh -c "uname -a; env"

Kubernetes Containers: Decomposing Container Images
 Step 3 of 4 
Dive
Alex Goodman has created a helpful tool for exploring layers in a container image called Dive. You can use it interactively, or add it to your CI pipeline to break builds when it finds bloated and inefficient containers.

Dive
Dive is a tool for exploring a container image, viewing layer contents, and discovering ways to shrink the size of your OCI image.

Return to the default directory and install the Dive tool.

>> cd ~/

>> wget -q https://github.com/wagoodman/dive/releases/download/v0.9.2/dive_0.9.2_linux_amd64.deb

>> apt -qq install ./dive_0.9.2_linux_amd64.deb

Verify it's installed and working.

>> dive --version

Dive runs in two modes: with an interactive text user interface (TUI) in the shell, or as a command-line tool tuned for integration into your continuous integration pipelines.

Interactive
To see the tool in action, let's use Dive's text user interface to look at the internals of the Redis container image created during the last step.

>> dive redis

In the interactive TUI, there are three panels; Layers, Current Layer Contents, and Image Details. The top-left is the listing of container layers. Move the highlighter up and down with the arrow keys to select a layer. Each layer's contents appear on the right-side panel. Press the Tab key to switch between the layer view (left) and the layer's file tree (right). There are key command hints at the bottom and more key bindings can be found here [https://github.com/wagoodman/dive#keybindings]. Take a moment to explore the tool.

The tool also allows you to select and delete files or directories from the layers. This can help you understand how to trim your containers. Dive can be a handy tool for experimenting, but you should refrain from using this UI as part of your continuous delivery process. The real trimming should happen when you define the containers with infrastructure-as-code.

The third panel in the interactive mode provides a summary of the efficiency and potential waste found.

To exit the interactive mode, use this clear to break out of the watch or press +.

Inspect the empty container image and verify it's truly empty.

>> dive empty

Inspect the BusyBox container image.

>> dive busybox

Notice that all the commands are present and the image efficiency is at 100%. Next, let's look at some less efficient containers.

You can explore other containers from other registries.

>> dive bitnami/kafka:2.5.0


Dive with Pipelines:
This tool can also be added to your CI/CD pipelines to alert you when it encounters inefficient and bloated containers that surpass your defined thresholds.

Let's build two containers with the same Python code to illustrate the process for tuning containers.

>> $ cat Dockerfile-a
>>>
# The wrong way to package a Python application in a container
FROM python:3

COPY fibonacci.py /

RUN pip install flask

CMD [ "python", "./fibonacci.py", "10" ]
> $ cat Dockerfile-b
>>>
FROM python:3.8.3-alpine3.11

COPY requirements.txt /tmp/

RUN pip install -r /tmp/requirements.txt

RUN adduser -D appuser
WORKDIR /home/appuser
USER appuser

COPY fibonacci.py .

ENTRYPOINT [ "python", "./fibonacci.py" ]
CMD ["10"]

[useful source : https://pythonspeed.com/articles/dockerizing-python-is-hard/]

Bloated Container (a)
The first container will be a fat container using a Dockerfile that has several bad practices.

>> ccat -l dockerfile Dockerfile-a

This example was inspired by the article, "Broken by default: why you should avoid most Dockerfile examples," by Itamar Turner-Trauring, where the bad practices are outlined.

Build the container.

>> docker build -t fibonacci-a --file Dockerfile-a .

Run the container.

>> docker run fibonacci-a

From this, the first ten Fibonacci numbers are produced. Let's see what the Dive tool thinks of this container. Generate the report.

>> dive fibonacci-a -j dive-report-a.json

View the report.

>> cat dive-report-a.json | jq .

There are many details, so instead pick out some key findings.
>> 
echo "              Size: $(cat dive-report-a.json | jq .image.sizeBytes) bytes" && \
echo "       Inefficient: $(cat dive-report-a.json | jq .image.inefficientBytes) bytes" && \
echo "Inefficiency score: $(cat dive-report-a.json | jq .image.efficiencyScore)"


Trimmed Container (b)
The Dockerfile-b is a typical revision of a Dockerfile for Python with several best practices applied.

>> ccat -l dockerfile Dockerfile-b

There are more best (or better) practices, tuning, and opinions that can be applied, but let's start with this.

Build the container.

>> docker build -t fibonacci-b --file Dockerfile-b .

Run the container.

>> docker run fibonacci-b

Let's see what the Dive tool thinks of this container. Generate the report.

>> dive fibonacci-b -j dive-report-b.json

With both container reports generated, compare the key findings.
>>>
echo "Container Image a" && \
echo "              Size: $(cat dive-report-a.json | jq .image.sizeBytes) bytes" && \
echo "       Inefficient: $(cat dive-report-a.json | jq .image.inefficientBytes) bytes" && \
echo "Inefficiency score: $(cat dive-report-a.json | jq .image.efficiencyScore)" && \
echo "" && echo "Container Image b" && \
echo "              Size: $(cat dive-report-b.json | jq .image.sizeBytes) bytes" && \
echo "       Inefficient: $(cat dive-report-b.json | jq .image.inefficientBytes) bytes" && \
echo "Inefficiency score: $(cat dive-report-b.json | jq .image.efficiencyScore)"
With this tool you can add thresholds to these values. You can be notified or force your pipeline to stop when newly built containers exceed your acceptable limits. This in turn can save you money by not wasting cloud resources (CPU, memory, and I/O) when your containers scale across your cluster.


Multi Stage Docker file:
------------------------
> read about pros of multistage docker file
> dont include package managers in docker file like, pip ,yum,apt
> dont build with :latest tag, since latest will get updated (go with particular version)
> in kubernets we can specify how much memory a pod can use 
> what is difference between annotations and labels??
> we can connect volumes to inside container,filesystem.kubernetes doesnt differentiate what kind of volume.
> Default kube memory,cpu configs works better than custom configs.

Pod disruption budget:
 > defines number of disruptions tolerated at a given time for a class of pods

> node affinity/Anti affinity, we can make a pod come up on particular node eg: cpu,gpu

> nodes are marked to tell scheduler what pods they prefer or not.

> pod are marked to tell scheduler what other pod types they wish to be near(cloud zones) or not


> go throught the ppt form kubernetes in three week of session2 where there is slides about liveness probes,rbac, sercurity,resource management











































































