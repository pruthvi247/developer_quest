	=========================================================
https://azure.microsoft.com/mediahandler/files/resourcefiles/phippy-goes-to-the-zoo/Phippy%20Goes%20To%20The%20Zoo_MSFTonline.pdf
=========================================================

pod : 
> pods are the basic unit of running containers inside of kubernetes
> A pod provides a way to set env variables,mount storages and feed other information  in to container
> Every Pod holds at least one container,and controls the execution of that container. When the containers exit, the Pod dies too.

Replica sets :
> A ReplicaSet ensures that a set of identically configured Pods are running at the desired replica count. If a Pod
drops off, the ReplicaSet brings a new one online as a replacement.

> Secrets are used to store non-public information, such as tokens, certificates, or passwords. Secrets can
be attached to Pods at runtime so that sensitive configuration data can be stored securely in the cluster

> DaemonSets provide a way to ensure that a copy of a Pod is running on every node in the cluster. As a
cluster grows and shrinks, the DaemonSet spreads these specially labeled Pods across all of the nodes

> A Deployment is a higher-order abstraction that controls deploying and maintaining a set of Pods. Behind
the scenes, it uses a ReplicaSet to keep the Pods running, but it offers sophisticated logic for deploying,
updating, and scaling a set of Pods within a cluster.

> Ingresses provide a way to declare that traffic ought to be channeled from the outside of the cluster into
destination points within the cluster. One single external Ingress point can accept traffic destined to
many different internal services.


> CronJobs provide a method for scheduling the execution of Pods. They are excellent for running periodic
tasks like backups, reports, and automated tests.

> CustomResourceDefinitions, or CRDs, provide an extension mechanism that cluster operators and
developers can use to create their own resource types.



==================================

> deployments uses replica sets
> virtual kublet provides virtual node, k8s api will talk to those nodes created by kublet


================= ######## kuberneties in action ########### ==================

$ docker run --name kubia-container -p 8080:8080 -d kubia

 -d : will run in background

$ docker inspect <container-name>

Running a shell inside an existing container:
	$ docker exec -it <container-name> bash
		-i, which makes sure STDIN 
		-t, pseudo tty
Tagging an image under an additional tag:
	$ docker tag kubia luksa/kubia
			- This doesn’t rename the tag; it creates an additional tag for the same image
> group of containers is called a Pod
> each pod has its own IP and contains one or more containers, each running an application process

Listing pods:
  $kubectl get pods
  	To see more information about the pod, you can also use the 
  	$kubectl describe pod <pod-name>

Creating a Service object: 
> To create the service, you’ll tell Kubernetes to expose the ReplicationController you created earlier:
		$ kubectl expose rc kubia --type=LoadBalancer --name kubia-http
Listing services:
	$kubectl get services

> kubectl run dockerhubrcnew --image=pruthvikumar247/kube_nodejs_image --port=8080 --generator=run/v1

> When a service is created, it gets a static IP, which never changes during the lifetime of the service. Instead of connecting to pods directly, clients should connect to the service through its constant IP address. The service makes sure one of the pods receives the connection, regardless of where the pod is currently running (and what its IP address is).

> To scale up the number of replicas of your pod, you need to change the desired replica count on the ReplicationController like this:
 	$ kubectl scale rc kubia --replicas=3
 		> to see replica count : kubectl get rc

> kubectl get pods -o wide (to get the details of the node where pod is running)


> To access url using minicube : $ minikube service kubia-http

> processes running in containers of the same pod need to take care not to bind to the same port numbers or they’ll run into port conflicts. But this only concerns containers in the same pod. Containers of different pods can never run into port conflicts, because each pod has a separate port space. All the containers in a pod also have the same loopback network interface, so a container can communicate with other containers in the same pod through localhost.

>  To get the yaml description of pod : $ kubectl get pods dockerhubrcnew-6jzrd -o yaml

> The kubectl create -f command is used for creating any resource (not only pods) from a YAML or JSON file.

$ docker logs <container id>
$ kubectl logs kubia-manual
$ kubectl logs <pod-name> -c <container-name>

$ kubectl port-forward kubia-manual 8888:8080 
			> command will forward your machine’s local port 8888 to port 8080 of your kubia-manual pod

> A label is an arbitrary key-value pair you attach to a resource, which is then utilized when selecting resources using label selectors

$ kubectl get po --show-labels
$ kubectl get po -L creation_method,env (get only specific labels)

Modifying labels of existing pods: 
		$ kubectl label po kubia-manual creation_method=manual
		$ kubectl label po kubia-manual-v2 env=debug -- (--overwrite option when changing existing labels)

Listing pods using a label selector:

	$ kubectl get po -l <label-name>

> Labels can be attached to any Kubernetes object, including nodes

		$kubectl label node gke-kubia-85f6-node-0rrx gpu=true

Annotating PODS:

> A great use of annotations is adding descriptions for each pod or other API object, so that everyone using the cluster can quickly look up information about each individual object. For example, an annotation used to specify the name of the person who created the object can make collaboration between everyone working on the cluster much easier.


> Annotations are also key-value pairs, so in essence, they’re similar to labels, but they aren’t meant to hold identifying information. They can’t be used to group objects the way labels can. While objects can be selected through label selectors, there’s no such thing as an annotation selector.

USING NAMESPACES TO GROUP RESOURCES :

To list pods in that namespace only:
			$ kubectl get po --namespace kube-system

> Besides isolating resources, namespaces are also used for allowing only certain users access to particular resources and even for limiting the amount of computational resources available to individual users
> When listing, describing, modifying, or deleting objects in other namespaces, you need to pass the --namespace (or -n) flag to kubectl

Switch between name sparce : $ kubectl config set-context $(kubectl config current-context) --namespace=<namespace>

> Kubernetes can check if a container is still alive through liveness probes


- Be sure to check only the internals of the app and nothing influenced by an external factor. For example, a frontend web server’s liveness probe shouldn’t return a failure when the server can’t connect to the backend database. If the underlying cause is in the database itself, restarting the web server container will not fix the problem. Because the liveness probe will fail again, you’ll end up with the container restarting repeatedly until the database becomes accessible again.

>  kubectl create -f kubia-liveness-probe.yaml


A ReplicationController has three essential parts :

A label selector, which determines what pods are in the ReplicationController’s scope
A replica count, which specifies the desired number of pods that should be running
A pod template, which is used when creating new pod replicas


Creating Replication controller :

apiVersion: v1
kind: ReplicationController        
metadata:
  name: kubia                     
spec:
  replicas: 3                      
  selector:                        
    app: kubia  ##(better not to specify selector, check below point)                   
  template:                        
    metadata:                      
      labels:                      
        app: kubia                 
    spec:                          
      containers:                  
      - name: kubia                
        image: luksa/kubia         
        ports:                     
        - containerPort: 8080      


> point : The pod labels in the template must obviously match the label selector of the ReplicationController; otherwise the controller would create new pods indefinitely, because spinning up a new pod wouldn’t bring the actual replica count any closer to the desired number of replicas

Moving pods in and out of the scope of a ReplicationController:

		> kubectl label pod kubia-dmdck type=special

Now, you’ll change the app=kubia label to something else. This will make the pod no longer match the ReplicationController’s label selector, leaving it to only match two pods. The ReplicationController should therefore start a new pod to bring the number back to three:
		> kubectl label pod kubia-dmdck app=foo --overwrite
							--overwrite argument is necessary; otherwise kubectl will only print out a warning and won’t change the label

Listing all the pods again:
	> kubectl get pods -L app


Changing the pod template:
> kubectl edit rc kubia
Configuring kubectl edit to use a different text editor:
> export KUBE_EDITOR="/usr/bin/nano"

Scaling up a ReplicationController:
> kubectl scale rc kubia --replicas=10 (we can also change by editing "kubectl edit rc kubia")

> Kubectl delete rc kubia --cascade=false

		-you can keep its pods running by passing the --cascade=false
Using a DaemonSet to run a pod on every node:
> A DaemonSet makes sure it creates as many pods as there are nodes and deploys each one on its own node
> Whereas a ReplicaSet (or ReplicationController) makes sure that a desired number of pod replicas exist in the cluster


	Chapter 5. Services: enabling clients to discover and talk to pods
	------------------------------------------------------------------

> The easiest way to create a service is through kubectl expose

Throught yaml :

apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  ports:
  - port: 80                
    targetPort: 8080        
  selector:                 
    app: kubia  

> kubectl get svc

The kubectl exec command allows you to remotely run arbitrary commands inside an existing container of a pod. This comes in handy when you want to examine the contents, state, and/or environment of a container.
		> kubectl exec kubia-7nog1 -- curl -s http://10.111.249.153
		(--)Everything after the double dash is the command that should be executed inside the pod.

> if you want all requests made by a certain client to be redirected to the same pod every time, you can set the service’s sessionAffinity property to ClientIP (instead of None, which is the default)
			apiVersion: v1
			kind: Service
			spec:
  				sessionAffinity: ClientIP
multiport config :

> if your pods listened on two ports—let’s say 8080 for HTTP and 8443 for HTTPS—you could use a single service to forward both port 80 and 443
---
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  ports:
  - name: http              
    port: 80                
    targetPort: 8080        
  - name: https             
    port: 443               
    targetPort: 8443        
  selector:                  
    app: kubia        				

> The label selector applies to the service as a whole—it can’t be configured for each port individually. If you want different ports to map to different subsets of pods, you need to create two services.

>  you can also give a name to each pod’s port and refer to it by name in the service spec.But why should you even bother with naming ports? The biggest benefit of doing so is that it enables you to change port numbers later without having to change the service spec.If you’re using named ports, all you need to do is change the port number in the pod spec (while keeping the port’s name unchanged)

 Discovering services:

 you can delete all pods without specifying their names like this: 
 	> kubectl delete po --all
 command to check environment variable of a pod:
 	> kubectl exec kubia-3inly env

> kubectl exec -it kubia-3inly bash
> minikube addons list
> minikube addons enable ingress
> kubectl get pod --all-namespaces
> kubectl get ingresses

> Liveness probes keep pods healthy by killing off unhealthy containers and replacing them with new, healthy ones, whereas readiness probes make sure that only pods that are ready to serve requests receive them. This is mostly necessary during container start up, but it’s also useful after the container has been running for a while.

		chapter6:Volumes: attaching disk storage to containers
		------------------------------------------------------

> Kubernetes volumes are a component of a pod and are thus defined in the pod’s specification—much like containers. They aren’t a standalone Kubernetes object and cannot be created or deleted on their own. A volume is available to all containers in the pod, but it must be mounted in each container that needs to access it. In each container, you can mount the volume in any location of its filesystem.

emptydirvolume : 
> the volume’s contents are lost when the pod is deleted.

you can tell Kubernetes to create the emptyDir on a tmpfs filesystem (in memory instead of on disk). To do this, set the emptyDir’s medium to Memory like this:
 > volumes:
  - name: html
    emptyDir:
      medium: Memory  
> normal way to create volumes : 
	volumeMounts:                        
    - name: html                           
      mountPath: /usr/share/nginx/html     
      readOnly: true

 Using a Git repository as the starting point for a volume :
  volumes:
  - name: html
    gitRepo:                                                           1
      repository: https://github.com/luksa/kubia-website-example.git   2
      revision: master                                                 3
      directory: . 

1 You’re creating a gitRepo volume.
2 The volume will clone this Git repository.
3 The master branch will be checked out.
4 You want the repo to be cloned into the root dir of the volume.



> To find an existing container image, which keeps a local directory synchronized with a Git repository, go to Docker Hub and search for “git sync.” You’ll find many images that do that. Then use the image in a new container in the pod from the previous example, mount the pod’s existing gitRepo volume in the new container, and configure the Git sync container to keep the files in sync with your Git repo. If you set everything up correctly, you should see that the files the web server is serving are kept in sync with your GitHub repo.


> If you want to clone a private Git repo into your container, you should use a gitsync sidecar or a similar method instead of a gitRepo volume.

 Introducing the hostPath volume:

 > A hostPath volume points to a specific file or directory on the node’s filesystem (see figure 6.4). Pods running on the same node and using the same path in their hostPath volume see the same files.

 > both the gitRepo and emptyDir volumes’ contents get deleted when a pod is torn down, whereas a hostPath volume’s contents don’t. If a pod is deleted and the next pod uses a hostPath volume pointing to the same path on the host, the new pod will see whatever was left behind by the previous pod, but only if it’s scheduled to the same node as the first pod  

 > kubectl explain command is used to show documentation about Kubernetes resources like pod. $ kubectl explain Pods

 Introducing PersistentVolumes and PersistentVolumeClaims:

 > PersistentVolumes are provisioned by cluster admins and consumed by pods through PersistentVolumeClaims
 > storageclasses
 > presistent volumes with out storage classes
 > gitRepo volumes
 > nfs volumes
## this chapter-6 need to be revised again, a lot of good info is present



chatpter 7 : ConfigMaps and Secrets: configuring applications:
-------------------------------------------------------------
> The Kubernetes resource for storing configuration data is called a ConfigMap
In a Dockerfile, two instructions define the two parts:

  > ENTRYPOINT defines the executable invoked when the container is started.
  > CMD specifies the arguments that get passed to the ENTRYPOINT.
> you should always use the exec form of the ENTRYPOINT instruction,to execute commands instead of CMD,as CMD creates extra shell process

> If you define two environment variables, the second one can include the value of the first one as shown in the following listing
    env:
      - name: FIRST_VAR
        value: "foo"
      - name: SECOND_VAR
        value: "$(FIRST_VAR)bar"

> An application doesn’t need to read the ConfigMap directly or even know that it exists. The contents of the map are instead passed to containers as either environment variables or as files in a volume (see figure 7.2). And because environment variables can be referenced in command-line arguments using the $(ENV_VAR) syntax, you can also pass ConfigMap entries to processes as command-line arguments.the application can also read the contents of a ConfigMap directly through the Kubernetes REST API endpoint if needed, but unless you have a real need for this, you should keep your app Kubernetes-agnostic as much as possible.
> $  kubectl create configmap fortune-config --from-literal=sleep-interval=25
          - This creates a ConfigMap called fortune-config with the single-entry sleep-interval =25

> ConfigMaps usually contain more than one entry. To create a ConfigMap with multiple literal entries, you add multiple --from-literal arguments:

        $ kubectl create configmap myconfigmap
            --from-literal=foo=bar --from-literal=bar=baz --from-literal=one=two

> inspect the YAML descriptor of the ConfigMap : 
        $ kubectl get configmap fortune-config -o yaml


> When creating ConfigMaps, you can use a combination of all the options mentioned here:

                $ kubectl create configmap my-config
                    --from-file=foo.json                     1
                    --from-file=bar=foobar.conf              2
                    --from-file=config-opts/                 3
                    --from-literal=some=thing                4
Description :
1 A single file
2 A file stored under a custom key
3 A whole directory
4 A literal value

Passing a ConfigMap entry to a container as an environment variable:

apiVersion: v1
kind: Pod
metadata:
  name: fortune-env-from-configmap
spec:
  containers:
  - image: luksa/fortune:env
    env:                             1
    - name: INTERVAL                 1
      valueFrom:                     2
        configMapKeyRef:             2
          name: fortune-config       3
          key: sleep-interval        4


> The container referencing the non-existing ConfigMap will fail to start
>   You can also mark a reference to a ConfigMap as optional (by setting configMapKeyRef.optional: true). In that case, the container starts even if the ConfigMap doesn’t exist.

Passing all entries of a ConfigMap as environment variables at once:
> Imagine having a ConfigMap with three keys called FOO, BAR, and FOO-BAR. You can expose them all as environment variables by using the envFrom attribute, instead of env the way

spec:
  containers:
  - image: some-image
    envFrom:                      1
    - prefix: CONFIG_             2
      configMapRef:               3
        name: my-config-map       3


Passing a ConfigMap entry as a command-line argument:

>  let’s also look at how to pass values from a ConfigMap as arguments to the main process running in the container. You can’t reference ConfigMap entries directly in the pod.spec.containers.args field, but you can first initialize an environment variable from the ConfigMap entry and then refer to the variable inside the arguments 

apiVersion: v1
kind: Pod
metadata:
  name: fortune-args-from-configmap
spec:
  containers:
  - image: luksa/fortune:args          1
    env:                               2
    - name: INTERVAL                   2
      valueFrom:                       2
        configMapKeyRef:               2
          name: fortune-config         2
          key: sleep-interval          2
    args: ["$(INTERVAL)"]              3

A pod with ConfigMap entries mounted as files: fortune-pod-configmap-volume.yaml:


apiVersion: v1
kind: Pod
metadata:
  name: fortune-configmap-volume
spec:
  containers:
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    ...
    - name: config
      mountPath: /etc/nginx/conf.d      1
      readOnly: true
    ...
  volumes:
  ...
  - name: config
    configMap:                          2
      name: fortune-config              2
  ...
Description:
1 You’re mounting the configMap volume at this location.
2 The volume refers to your fortune-config ConfigMap.

 A pod with a specific ConfigMap entry mounted into a file directory: fortune-pod-configmap-volume-with-items.yaml:

 volumes:
  - name: config
    configMap:
      name: fortune-config
      items:                             1
      - key: my-nginx-config.conf        2
        path: gzip.conf                  3
Description:
1 Selecting which entries to include in the volume by listing them
2 You want the entry under this key included.
3 The entry’s value should be stored in this file.

> what happens in Linux when you mount a filesystem into a nonempty directory. The directory then only contains the files from the mounted filesystem, whereas the original files in that directory are inaccessible for as long as the filesystem is mounted.

Mounting individual ConfigMap entries as files without hiding oth- her files in the directory:

> Naturally, you’re now wondering how to add individual files from a ConfigMap into an existing directory without hiding existing files stored in it. An additional subPath property on the volumeMount allows you to mount either a single file or a single directory from the volume instead of mounting the whole volume

> Say you have a configMap volume containing a myconfig.conf file, which you want to add to the /etc directory as someconfig.conf. You can use the subPath property to mount it there without affecting any other files in that directory. 

spec:
  containers:
  - image: some/image
    volumeMounts:
    - name: myvolume
      mountPath: /etc/someconfig.conf      1
      subPath: myconfig.conf               2
Description:
1 You’re mounting into a file, not a directory.
2 Instead of mounting the whole volume, you’re only mounting the myconfig.conf entry.


Setting file permissions: fortune-pod-configmap-volume-defaultMode.yaml:
  volumes:
  - name: config
    configMap:
      name: fortune-config
      defaultMode: "6600"     


> Using a ConfigMap and exposing it through a volume brings the ability to update the configuration without having to recreate the pod or even restart the container.
> $ kubectl edit configmap fortune-config
> $  kubectl exec fortune-configmap-volume -c web-server
  cat /etc/nginx/conf.d/my-nginx-config.conf
> kubectl exec fortune-configmap-volume -c web-server -- nginx -s reload
> One big caveat relates to updating ConfigMap-backed volumes. If you’ve mounted a single file in the container instead of the whole volume, the file will not be updated! At least, this is true at the time of writing this chapter.

Introducing the default token Secret:

Volumes:
  default-token-cfee9:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-cfee9
> Every pod has a secret volume attached to it automatically. The volume in the previous kubectl describe output refers to a Secret called default-token-cfee9. Because Secrets are resources, you can list them with kubectl get secrets

> $ kubectl describe secrets

Creating a Secret:
> $ kubectl create secret generic fortune-https --from-file=https.key
  --from-file=https.cert --from-file=foo

  > This isn’t very different from creating ConfigMaps. In this case, you’re creating a generic Secret called fortune-https and including two entries in it (https.key with the contents of the https.key file and likewise for the https.cert key/file). As you learned earlier, you could also include the whole directory with --from-file=fortune-https instead of specifying each file individually.

Mounting the fortune-https Secret in a pod:

Listing 7.25. YAML definition of the fortune-https pod: fortune-pod-https.yaml
apiVersion: v1
kind: Pod
metadata:
  name: fortune-https
spec:
  containers:
  - image: luksa/fortune:env
    name: html-generator
    env:
    - name: INTERVAL
      valueFrom:
        configMapKeyRef:
          name: fortune-config
          key: sleep-interval
    volumeMounts:
    - name: html
      mountPath: /var/htdocs
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    - name: config
      mountPath: /etc/nginx/conf.d
      readOnly: true
    - name: certs                         1
      mountPath: /etc/nginx/certs/        1
      readOnly: true                      1
    ports:
    - containerPort: 80
    - containerPort: 443
  volumes:
  - name: html
    emptyDir: {}
  - name: config
    configMap:
      name: fortune-config
      items:
      - key: my-nginx-config.conf
        path: https.conf
  - name: certs                            2
    secret:                                2
      secretName: fortune-https            2

Exposing a Secret’s entry as an environment variable:
env:
    - name: FOO_SECRET
      valueFrom:                   1
        secretKeyRef:              1
          name: fortune-https      2
          key: foo                 3

Chapter 8. Accessing pod metadata and other resources from applications
-----------------------------------------------------------------------
> service account is the account that the pod authenticates as when talking to the API server
Understanding the available metadata:
> The Downward API enables you to expose the pod’s own metadata to the processes running inside that pod.
> pass the pod’s and container’s metadata to the container through environment variables:
> If you prefer to expose the metadata through files instead of environment variables, you can define a downwardAPI volume and mount it into your container. You must use a downwardAPI volume for exposing the pod’s labels or its annotations
> As with the configMap and secret volumes, you can change the file permissions through the downwardAPI volume’s defaultMode property in the pod spec.
> But the metadata available through the Downward API is fairly limited. If you need more, you’ll need to obtain it from the Kubernetes API server directly. 
> The kubectl proxy command runs a proxy server that accepts HTTP connections on your local machine and proxies them to the API server while taking care of authentication
> Never skip checking the server’s certificate in an actual application. Doing so could make your app expose its authentication token to an attacker using a man-in-the-middle attack.
> 
